{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authenticating our Python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{\"created_at\": \"Fri Jul 03 14:08:13 +0000 2020\", \"default_profile\": true, \"default_profile_image\": true, \"followers_count\": 1, \"id\": 1279054229819154433, \"id_str\": \"1279054229819154433\", \"name\": \"Darsh Chandak\", \"profile_background_color\": \"F5F8FA\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png\", \"profile_image_url_https\": \"https://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_border_color\": \"C0DEED\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"profile_use_background_image\": true, \"screen_name\": \"ChandakDarsh\"}\n"
    }
   ],
   "source": [
    "import twitter\n",
    "twitter_api = twitter.Api(consumer_key='YCItuzSp8kdKBq81Tj4mmGBZm',\n",
    "                  consumer_secret='fb1ZnmFVKTG54wpAamnxNt7GRaFa9vYBIvUAuCoAfaQWTw5Epd',\n",
    "                  access_token_key='1279054229819154433-1thc23GnRx4Hrh0DR6xTKhBhVXIQAv',\n",
    "                  access_token_secret='GI8oRzX0Ir68JAVUT4IqK7dKtx6uiedo0NxQlN8RWxjyL')\n",
    "\n",
    "\n",
    "# test authentication\n",
    "verify = twitter_api.VerifyCredentials()\n",
    "print(verify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the function to build the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def buildTestSet(search_keyword):\n",
    "    try:\n",
    "        tweets_fetched = twitter_api.GetSearch(search_keyword, count = 100)\n",
    "        \n",
    "        print(\"Fetched \" + str(len(tweets_fetched)) + \" tweets for the term \" + search_keyword)\n",
    "        \n",
    "        return [{\"Tweet\":status.text, \"Label\":None} for status in tweets_fetched]\n",
    "    except:\n",
    "        print(\"Unfortunately, something went wrong..\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fetched 100 tweets for the term Covid-19\n[{'Tweet': \"Schools can't even control lice, you think they can control Covid-19.\", 'Label': None}, {'Tweet': 'Your reminder that schools in Germany, Norway, and Denmark can open safely because their leaders contained Covid-19… https://t.co/YrQ9jrWW0q', 'Label': None}, {'Tweet': 'The media cannot admit that we have effective COVID-19 treatments because then they wouldn’t be able to continue th… https://t.co/MO92AeLbN7', 'Label': None}, {'Tweet': 'RT @lawancovid19_id: Orang tua memiliki peran sentral dalam melindungi anak dari COVID-19.\\n\\nIkatan Dokter Anak Indonesia telah mengeluarkan…', 'Label': None}]\n"
    }
   ],
   "source": [
    "search_term = \"Covid-19\"\n",
    "testDataSet = buildTestSet(search_term)\n",
    "\n",
    "print(testDataSet[0:4])\n",
    "# print(tweet[\"text\"]+\"  \"+tweet[\"label\"])\n",
    "# print(testDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[{'Tweet_id': '126415614616154112', 'Tweet': 'Now all @Apple has to do is get swype on the iphone and it will be crack. Iphone that is', 'Label': 'positive', 'Topic': 'apple'}, {'Tweet_id': '126402758403305474', 'Tweet': \"Hilarious @youtube video - guy does a duet with @apple 's Siri. Pretty much sums up the love affair! http://t.co/8ExbnQjY\", 'Label': 'positive', 'Topic': 'apple'}, {'Tweet_id': '126397179614068736', 'Tweet': '@RIM you made it too easy for me to switch to @Apple iPhone. See ya!', 'Label': 'positive', 'Topic': 'apple'}, {'Tweet_id': '126379685453119488', 'Tweet': 'The 16 strangest things Siri has said so far. I am SOOO glad that @Apple gave Siri a sense of humor! http://t.co/TWAeUDBp via @HappyPlace', 'Label': 'positive', 'Topic': 'apple'}]\n"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('Datasets/tweetDataFile2.csv', newline='',encoding=\"utf8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    trainingData = []\n",
    "    for row in reader:\n",
    "        trainingData.append({\"Tweet_id\":row[0], \"Tweet\":row[1], \"Label\":row[2], \"Topic\":row[3]})\n",
    "        # your_list = list(reader)\n",
    "\n",
    "print(trainingData[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "class PreProcessTweets:\n",
    "    def __init__(self):\n",
    "        self._stopwords = set(stopwords.words('english') + list(punctuation) + ['AT_USER','URL'])\n",
    "        \n",
    "    def processTweets(self, list_of_tweets):\n",
    "        processedTweets=[]\n",
    "        for tweet in list_of_tweets:\n",
    "            processedTweets.append((self._processTweet(tweet[\"Tweet\"]),tweet[\"Label\"]))\n",
    "        return processedTweets\n",
    "    \n",
    "    def _processTweet(self, tweet):\n",
    "        tweet = tweet.lower() # convert text to lower-case\n",
    "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet) # remove URLs\n",
    "        tweet = re.sub('@[^\\s]+', 'AT_USER', tweet) # remove usernames\n",
    "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) # remove the # in #hashtag\n",
    "        tweet = word_tokenize(tweet) # remove repeated characters (helloooooooo into hello)\n",
    "        return [word for word in tweet if word not in self._stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(['get', 'swype', 'iphone', 'crack', 'iphone'], 'positive'), (['hilarious', 'video', 'guy', 'duet', \"'s\", 'siri', 'pretty', 'much', 'sums', 'love', 'affair'], 'positive'), (['made', 'easy', 'switch', 'iphone', 'see', 'ya'], 'positive'), (['16', 'strangest', 'things', 'siri', 'said', 'far', 'sooo', 'glad', 'gave', 'siri', 'sense', 'humor', 'via'], 'positive')]\n"
    }
   ],
   "source": [
    "tweetProcessor = PreProcessTweets()\n",
    "preprocessedTrainingSet = tweetProcessor.processTweets(trainingData)\n",
    "print(preprocessedTrainingSet[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(['schools', 'ca', \"n't\", 'even', 'control', 'lice', 'think', 'control', 'covid-19'], None), (['reminder', 'schools', 'germany', 'norway', 'denmark', 'open', 'safely', 'leaders', 'contained', 'covid-19…'], None), (['media', 'admit', 'effective', 'covid-19', 'treatments', '’', 'able', 'continue', 'th…'], None), (['rt', 'orang', 'tua', 'memiliki', 'peran', 'sentral', 'dalam', 'melindungi', 'anak', 'dari', 'covid-19', 'ikatan', 'dokter', 'anak', 'indonesia', 'telah', 'mengeluarkan…'], None)]\n"
    }
   ],
   "source": [
    "# tweetProcessor = PreProcessTweets()\n",
    "preprocessedTestSet = tweetProcessor.processTweets(testDataSet)\n",
    "print(preprocessedTestSet[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "def buildVocabulary(preprocessedTrainingSet):\n",
    "    all_words = []\n",
    "    \n",
    "    for (words, sentiment) in preprocessedTrainingSet:\n",
    "        all_words.extend(words)\n",
    "\n",
    "    wordlist = nltk.FreqDist(all_words)\n",
    "    word_features = wordlist.keys()\n",
    "    \n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_features = buildVocabulary(preprocessedTrainingSet)\n",
    "# print(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "trainingFeatures = nltk.classify.apply_features(extract_features, preprocessedTrainingSet)\n",
    "# print(trainingFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<nltk.classify.naivebayes.NaiveBayesClassifier object at 0x0856FDA8>\n"
    }
   ],
   "source": [
    "\n",
    "NBayesClassifier = nltk.NaiveBayesClassifier.train(trainingFeatures)\n",
    "print(NBayesClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Positive Sentiment Percentage = 0.0%\nNegative Sentiment Percentage = 0.0%\n"
    }
   ],
   "source": [
    "NBResultLabels = [NBayesClassifier.classify(extract_features(tweet[0])) for tweet in preprocessedTestSet]\n",
    "\n",
    "# # get the majority vote\n",
    "# if NBResultLabels.count('positive') > NBResultLabels.count('negative'):\n",
    "#     print(\"Overall Positive Sentiment\")\n",
    "#     print(\"Positive Sentiment Percentage = \" + str(100*NBResultLabels.count('positive')/len(NBResultLabels)) + \"%\")\n",
    "# else: \n",
    "#     print(\"Overall Negative Sentiment\")\n",
    "#     print(\"Negative Sentiment Percentage = \" + str(100*NBResultLabels.count('negative')/len(NBResultLabels)) + \"%\")\n",
    "\n",
    "\n",
    "print(\"Positive Sentiment Percentage = \" + str(100*NBResultLabels.count('positive')/len(NBResultLabels)) + \"%\")\n",
    "    \n",
    "print(\"Negative Sentiment Percentage = \" + str(100*NBResultLabels.count('negative')/len(NBResultLabels)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 32-bit",
   "language": "python",
   "name": "python_defaultSpec_1594359004482"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}